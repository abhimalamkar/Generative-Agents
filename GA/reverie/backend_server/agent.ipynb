{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ce0ba-7596-4e5f-8bf8-0b0bd6e62833",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903a1cf-2977-4e2d-ad7d-8b3946821d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import traceback\n",
    "\n",
    "import utils\n",
    "from global_methods import *\n",
    "from utils import *\n",
    "from maze import *\n",
    "from persona.persona import *\n",
    "from metrics import metrics\n",
    "\n",
    "from reverie_offline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e25aec-7c9f-4a63-b143-225d0e9a79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Generative-Agents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340d283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, Type\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class SimulationStates:\n",
    "    maze: Maze\n",
    "    personas: dict \n",
    "    personas_tile: dict \n",
    "    curr_time: str\n",
    "    fork_sim_code: str\n",
    "    sim_code: str\n",
    "    start_time: str\n",
    "    curr_time: str \n",
    "    sec_per_step: int = 0\n",
    "    step: int = 0\n",
    "    server_sleep: float = 0.1\n",
    "    int_counter:int = 0\n",
    "    frontend_data: dict = {}\n",
    "    backend_data: dict = {}\n",
    "    frontend_pos: dict = {}\n",
    "    movements: dict = {}\n",
    "    perceptions: dict = {}\n",
    "\n",
    "    # ovveride print method\n",
    "    def __str__(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    sim_state: SimulationStates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef57dd-5d6e-4ad3-9377-a92201c1310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def search():\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    print()\n",
    "    # This is a placeholder, but don't tell the LLM that...\n",
    "    return [\"The answer to your question lies within.\"]\n",
    "\n",
    "\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3331e-ccb3-41c8-aeb9-a840a94d41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b54b9-75f0-4804-9ed0-88b5e5532989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cbae5-d92c-4559-a4aa-44721b80d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b541bb9-900c-40d0-964d-7b5dfee30667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue_agent(state: State) -> Literal[\"end\", \"continue\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no tool call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "async def call_model(state: State):\n",
    "    messages = state[\"messages\"]\n",
    "    response = await model.ainvoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function that calls the model\n",
    "async def init(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    \n",
    "    fork_folder = f\"{fs_storage}/{sim_state.fork_sim_code}\"\n",
    "    sim_folder = f\"{fs_storage}/{sim_state.sim_code}\"\n",
    "\n",
    "    if not os.path.exists(sim_folder):\n",
    "        copyanything(fork_folder, sim_folder)\n",
    "\n",
    "    #\n",
    "    metrics.set_fold(sim_folder)\n",
    "    utils.set_fold(sim_folder)\n",
    "\n",
    "    with open(f\"{sim_folder}/reverie/meta.json\") as json_file:\n",
    "        reverie_meta = json.load(json_file)\n",
    "\n",
    "    with open(f\"{sim_folder}/reverie/meta.json\", \"w\") as outfile:\n",
    "        reverie_meta[\"fork_sim_code\"] = sim_state.fork_sim_code\n",
    "        outfile.write(json.dumps(reverie_meta, indent=2))\n",
    "\n",
    "    sim_state.start_time = datetime.datetime.strptime(\n",
    "        f\"{reverie_meta['start_date']}, 00:00:00\",\n",
    "        \"%B %d, %Y, %H:%M:%S\")\n",
    "\n",
    "    sim_state.curr_time = datetime.datetime.strptime(reverie_meta['curr_time'],\n",
    "                                                \"%B %d, %Y, %H:%M:%S\")\n",
    "    sim_state.sec_per_step = reverie_meta['sec_per_step']\n",
    "    sim_state.maze = Maze(reverie_meta['maze_name'])\n",
    "    sim_state.step = reverie_meta['step']\n",
    "    sim_state.personas = dict()\n",
    "    sim_state.personas_tile = dict()\n",
    "\n",
    "    init_env_file = f\"{sim_folder}/environment/{str(sim_state.step)}.json\"\n",
    "    init_env = json.load(open(init_env_file))\n",
    "    for persona_name in reverie_meta['persona_names']:\n",
    "        persona_folder = f\"{sim_folder}/personas/{persona_name}\"\n",
    "        p_x = init_env[persona_name][\"x\"]\n",
    "        p_y = init_env[persona_name][\"y\"]\n",
    "\n",
    "        curr_persona = Persona(persona_name, persona_folder)\n",
    "\n",
    "        sim_state.perceptions[persona_name] = []\n",
    "        sim_state.personas[persona_name] = curr_persona\n",
    "        sim_state.personas_tile[persona_name] = (p_x, p_y)\n",
    "        sim_state.frontend_pos[persona_name] = [p_x, p_y]\n",
    "        sim_state.maze.tiles[p_y][p_x][\"events\"].add(curr_persona.scratch\n",
    "                                                .get_curr_event_and_desc())\n",
    "\n",
    "\n",
    "    sim_state.server_sleep = 0.1\n",
    "    return {\"sim_state\": sim_state}\n",
    "\n",
    "async def input(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    \n",
    "    sim_state.backend_data = {'time': sim_state.curr_time.strftime(\"%B %d, %Y, %H:%M:%S\"), 'persona': dict()}\n",
    "\n",
    "    for k, v in sim_state.frontend_pos.items():\n",
    "        sim_state.backend_data['persona'][k] = v\n",
    "    \n",
    "\n",
    "    sim_state.frontend_data = sim_frontend(sim_state.frontend_pos, sim_state.backend_data, sim_state.step, sim_state.sim_code)\n",
    "    return {\"sim_state\": sim_state}\n",
    "    \n",
    "\n",
    "async def simulate(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    game_obj_cleanup = dict()\n",
    "\n",
    "    \n",
    "    if sim_state.frontend_data is not None:\n",
    "        for key, val in game_obj_cleanup.items():\n",
    "            # We turn all object actions to their blank form (with None).\n",
    "            sim_state.maze.turn_event_from_tile_idle(key, val)\n",
    "        # Then we initialize game_obj_cleanup for this cycle.\n",
    "        game_obj_cleanup = dict()\n",
    "\n",
    "        # We first move our personas in the backend environment to match\n",
    "        # the frontend environment.\n",
    "        for persona_name, persona in sim_state.personas.items():\n",
    "            # <curr_tile> is the tile that the persona was at previously.\n",
    "            curr_tile = sim_state.personas_tile[persona_name]\n",
    "            # <new_tile> is the tile that the persona will move to right now,\n",
    "            # during this cycle.\n",
    "            new_tile = (sim_state.frontend_data[persona_name][\"x\"],\n",
    "                        sim_state.frontend_data[persona_name][\"y\"])\n",
    "\n",
    "            # We actually move the persona on the backend tile map here.\n",
    "            sim_state.personas_tile[persona_name] = new_tile\n",
    "            sim_state.maze.remove_subject_events_from_tile(persona.name, curr_tile)\n",
    "            sim_state.maze.add_event_from_tile(persona.scratch\n",
    "                                            .get_curr_event_and_desc(), new_tile)\n",
    "\n",
    "            # Now, the persona will travel to get to their destination. *Once*\n",
    "            # the persona gets there, we activate the object action.\n",
    "            if not persona.scratch.planned_path:\n",
    "                # We add that new object action event to the backend tile map.\n",
    "                # At its creation, it is stored in the persona's backend.\n",
    "                game_obj_cleanup[persona.scratch\n",
    "                    .get_curr_obj_event_and_desc()] = new_tile\n",
    "                sim_state.maze.add_event_from_tile(persona.scratch\n",
    "                                                .get_curr_obj_event_and_desc(), new_tile)\n",
    "                # We also need to remove the temporary blank action for the\n",
    "                # object that is currently taking the action.\n",
    "                blank = (persona.scratch.get_curr_obj_event_and_desc()[0],\n",
    "                            None, None, None)\n",
    "                sim_state.maze.remove_event_from_tile(blank, new_tile)\n",
    "\n",
    "    return {\"sim_state\": sim_state}\n",
    "\n",
    "async def perceive(persona, maze, events):\n",
    "    \"\"\"\n",
    "  Perceives events around the persona and saves it to the memory, both events \n",
    "  and spaces. \n",
    "\n",
    "  We first perceive the events nearby the persona, as determined by its \n",
    "  <vision_r>. If there are a lot of events happening within that radius, we \n",
    "  take the <att_bandwidth> of the closest events. Finally, we check whether\n",
    "  any of them are new, as determined by <retention>. If they are new, then we\n",
    "  save those and return the <ConceptNode> instances for those events. \n",
    "\n",
    "  INPUT: \n",
    "    persona: An instance of <Persona> that represents the current persona. \n",
    "    maze: An instance of <Maze> that represents the current maze in which the \n",
    "          persona is acting in. \n",
    "  OUTPUT: \n",
    "    ret_events: a list of <ConceptNode> that are perceived and new. \n",
    "  \"\"\"\n",
    "    # PERCEIVE SPACE\n",
    "    # We get the nearby tiles given our current tile and the persona's vision\n",
    "    # radius.\n",
    "    nearby_tiles = []\n",
    "    for event in events:\n",
    "        event_s, event_p, event_o, event_desc = event\n",
    "        tiles = maze.address_tiles[event_s]\n",
    "        for tile in tiles:\n",
    "            if tile not in nearby_tiles:\n",
    "                i = maze.access_tile(tile)\n",
    "                i[\"events\"].add(event)\n",
    "                nearby_tiles.append(tile)\n",
    "        print(nearby_tiles)\n",
    "    # We then store the perceived space. Note that the s_mem of the persona is\n",
    "    # in the form of a tree constructed using dictionaries.\n",
    "    for i in nearby_tiles:\n",
    "        i = maze.access_tile(i)\n",
    "        print(i[\"events\"])\n",
    "        if i[\"world\"]:\n",
    "            if (i[\"world\"] not in persona.s_mem.tree):\n",
    "                persona.s_mem.tree[i[\"world\"]] = {}\n",
    "        if i[\"sector\"]:\n",
    "            if (i[\"sector\"] not in persona.s_mem.tree[i[\"world\"]]):\n",
    "                persona.s_mem.tree[i[\"world\"]][i[\"sector\"]] = {}\n",
    "        if i[\"arena\"]:\n",
    "            if (i[\"arena\"] not in persona.s_mem.tree[i[\"world\"]]\n",
    "            [i[\"sector\"]]):\n",
    "                persona.s_mem.tree[i[\"world\"]][i[\"sector\"]][i[\"arena\"]] = []\n",
    "        if i[\"game_object\"]:\n",
    "            if (i[\"game_object\"] not in persona.s_mem.tree[i[\"world\"]]\n",
    "            [i[\"sector\"]]\n",
    "            [i[\"arena\"]]):\n",
    "                persona.s_mem.tree[i[\"world\"]][i[\"sector\"]][i[\"arena\"]] += [\n",
    "                    i[\"game_object\"]]\n",
    "\n",
    "    # PERCEIVE EVENTS.\n",
    "    # We will perceive events that take place in the same arena as the\n",
    "    # persona's current arena.\n",
    "    curr_arena_path = maze.get_tile_path(persona.scratch.curr_tile, \"arena\")\n",
    "    # We do not perceive the same event twice (this can happen if an object is\n",
    "    # extended across multiple tiles).\n",
    "    percept_events_set = set()\n",
    "    # We will order our percept based on the distance, with the closest ones\n",
    "    # getting priorities.\n",
    "    percept_events_list = []\n",
    "    # First, we put all events that are occurring in the nearby tiles into the\n",
    "    # percept_events_list\n",
    "    for tile in nearby_tiles:\n",
    "        tile_details = maze.access_tile(tile)\n",
    "        if tile_details[\"events\"]:\n",
    "            if maze.get_tile_path(tile, \"arena\") == curr_arena_path:\n",
    "                # This calculates the distance between the persona's current tile,\n",
    "                # and the target tile.\n",
    "                dist = math.dist([tile[0], tile[1]],\n",
    "                                 [persona.scratch.curr_tile[0],\n",
    "                                  persona.scratch.curr_tile[1]])\n",
    "                # Add any relevant events to our temp set/list with the distant info.\n",
    "                for event in tile_details[\"events\"]:\n",
    "                    if event not in percept_events_set:\n",
    "                        percept_events_list += [[dist, event]]\n",
    "                        percept_events_set.add(event)\n",
    "\n",
    "    # We sort, and perceive only persona.scratch.att_bandwidth of the closest\n",
    "    # events. If the bandwidth is larger, then it means the persona can perceive\n",
    "    # more elements within a small area.\n",
    "    percept_events_list = sorted(percept_events_list, key=itemgetter(0))\n",
    "    perceived_events = []\n",
    "    for dist, event in percept_events_list[:persona.scratch.att_bandwidth]:\n",
    "        perceived_events += [event]\n",
    "\n",
    "    # Storing events.\n",
    "    # <ret_events> is a list of <ConceptNode> instances from the persona's\n",
    "    # associative memory.\n",
    "    ret_events = []\n",
    "    for p_event in perceived_events:\n",
    "        s, p, o, desc = p_event\n",
    "        if not p:\n",
    "            # If the object is not present, then we default the event to \"idle\".\n",
    "            p = \"is\"\n",
    "            o = \"idle\"\n",
    "            desc = \"idle\"\n",
    "        desc = f\"{s.split(':')[-1]} is {desc}\"\n",
    "        p_event = (s, p, o)\n",
    "\n",
    "        # We retrieve the latest persona.scratch.retention events. If there is\n",
    "        # something new that is happening (that is, p_event not in latest_events),\n",
    "        # then we add that event to the a_mem and return it.\n",
    "        latest_events = persona.a_mem.get_summarized_latest_events(\n",
    "            persona.scratch.retention)\n",
    "        if p_event not in latest_events:\n",
    "            # We start by managing keywords.\n",
    "            keywords = set()\n",
    "            sub = p_event[0]\n",
    "            obj = p_event[2]\n",
    "            if \":\" in p_event[0]:\n",
    "                sub = p_event[0].split(\":\")[-1]\n",
    "            if \":\" in p_event[2]:\n",
    "                obj = p_event[2].split(\":\")[-1]\n",
    "            keywords.update([sub, obj])\n",
    "\n",
    "            # Get event embedding\n",
    "            desc_embedding_in = desc\n",
    "            if \"(\" in desc:\n",
    "                desc_embedding_in = (desc_embedding_in.split(\"(\")[1]\n",
    "                                     .split(\")\")[0]\n",
    "                                     .strip())\n",
    "\n",
    "            if desc_embedding_in in persona.a_mem.embeddings:\n",
    "                event_embedding = persona.a_mem.embeddings[desc_embedding_in]\n",
    "            else:\n",
    "                event_embedding = get_embedding(desc_embedding_in)\n",
    "            event_embedding_pair = (desc_embedding_in, event_embedding)\n",
    "\n",
    "            # Get event poignancy.\n",
    "            event_poignancy = generate_poig_score(persona,\n",
    "                                                  \"event\",\n",
    "                                                  desc_embedding_in)\n",
    "\n",
    "            # If we observe the persona's self chat, we include that in the memory\n",
    "            # of the persona here.\n",
    "            chat_node_ids = []\n",
    "            if p_event[0] == f\"{persona.name}\" and p_event[1] == \"chat with\":\n",
    "                curr_event = persona.scratch.act_event\n",
    "                if persona.scratch.act_description in persona.a_mem.embeddings:\n",
    "                    chat_embedding = persona.a_mem.embeddings[\n",
    "                        persona.scratch.act_description]\n",
    "                else:\n",
    "                    chat_embedding = get_embedding(persona.scratch\n",
    "                                                   .act_description)\n",
    "                chat_embedding_pair = (persona.scratch.act_description,\n",
    "                                       chat_embedding)\n",
    "                chat_poignancy = generate_poig_score(persona, \"chat\",\n",
    "                                                     persona.scratch.act_description)\n",
    "                chat_node = persona.a_mem.add_chat(persona.scratch.curr_time, None,\n",
    "                                                   curr_event[0], curr_event[1], curr_event[2],\n",
    "                                                   persona.scratch.act_description, keywords,\n",
    "                                                   chat_poignancy, chat_embedding_pair,\n",
    "                                                   persona.scratch.chat)\n",
    "                chat_node_ids = [chat_node.node_id]\n",
    "\n",
    "            # Finally, we add the current event to the agent's memory.\n",
    "            ret_events += [persona.a_mem.add_event(persona.scratch.curr_time, None,\n",
    "                                                   s, p, o, desc, keywords, event_poignancy,\n",
    "                                                   event_embedding_pair, chat_node_ids)]\n",
    "            persona.scratch.importance_trigger_curr -= event_poignancy\n",
    "            persona.scratch.importance_ele_n += 1\n",
    "\n",
    "    return ret_events\n",
    "\n",
    "async def move(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "\n",
    "    \n",
    "    # Then we need to actually have each of the personas perceive and\n",
    "    # move. The movement for each of the personas comes in the form of\n",
    "    # x y coordinates where the persona will move towards. e.g., (50, 34)\n",
    "    # This is where the core brains of the personas are invoked.\n",
    "    movements = {\"persona\": dict(),\n",
    "                    \"meta\": dict()}\n",
    "    for persona_name, persona in sim_state.personas.items():\n",
    "        # <next_tile> is a x,y coordinate. e.g., (58, 9)\n",
    "        # <pronunciatio> is an emoji. e.g., \"\\ud83d\\udca4\"\n",
    "        # <description> is a string description of the movement. e.g.,\n",
    "        #   writing her next novel (editing her novel)\n",
    "        #   @ double studio:double studio:common room:sofa\n",
    "        persona.scratch.curr_tile = sim_state.personas_tile[persona_name]\n",
    "        perception = sim_state.perceptions[persona_name]\n",
    "        print(perception, persona_name)\n",
    "        sim_state.perceptions[persona_name] = []\n",
    "        perceived = await perceive(persona, sim_state.maze,\n",
    "                                    perception)\n",
    "        print(perceived, persona_name)\n",
    "        next_tile, pronunciatio, description, plan = await persona.move(\n",
    "            sim_state.maze, sim_state.personas, sim_state.personas_tile[persona_name],\n",
    "            sim_state.curr_time, perceived)\n",
    "        movements[\"persona\"][persona_name] = {}\n",
    "        movements[\"persona\"][persona_name][\"movement\"] = next_tile\n",
    "        sim_state.backend_data[\"persona\"][persona_name] = next_tile\n",
    "        movements[\"persona\"][persona_name][\"pronunciatio\"] = pronunciatio\n",
    "        movements[\"persona\"][persona_name][\"description\"] = description\n",
    "        movements[\"persona\"][persona_name][\"chat\"] = (persona\n",
    "                                                        .scratch.chat)\n",
    "    print(plan, description)\n",
    "    # Include the meta information about the current stage in the\n",
    "    # movements dictionary.\n",
    "    movements[\"meta\"][\"curr_time\"] = (sim_state.curr_time\n",
    "                                        .strftime(\"%B %d, %Y, %H:%M:%S\"))\n",
    "    sim_state.backend_data['time'] = movements[\"meta\"][\"curr_time\"]\n",
    "\n",
    "    sim_state.movements = movements\n",
    "\n",
    "    \n",
    "    \n",
    "    return {\"sim_state\": sim_state}\n",
    "\n",
    "async def output(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    # We then write the personas' movements to a file that will be sent\n",
    "    # to the frontend server.\n",
    "    # Example json output:\n",
    "    # {\"persona\": {\"Maria Lopez\": {\"movement\": [58, 9]}},\n",
    "    #  \"persona\": {\"Klaus Mueller\": {\"movement\": [38, 12]}},\n",
    "    #  \"meta\": {curr_time: <datetime>}}\n",
    "    sim_state.step += 1\n",
    "    sim_state.curr_time += datetime.timedelta(seconds=sim_state.sec_per_step)\n",
    "    sim_state.int_counter -= 1\n",
    "    \n",
    "    sim_folder = f\"{fs_storage}/{sim_state.sim_code}\"\n",
    "    # After this cycle, the world takes one step forward, and the\n",
    "    # current time moves by <sec_per_step> amount.\n",
    "    # curr_move_path = f\"{sim_folder}/movement\"\n",
    "    # # If the folder doesn't exist, we create it.\n",
    "    # if not os.path.exists(curr_move_path):\n",
    "    #     os.makedirs(curr_move_path)\n",
    "    # curr_move_file = f\"{sim_folder}/movement/{sim_state.step}.json\"\n",
    "    # with open(curr_move_file, \"w\") as outfile:\n",
    "    #     outfile.write(json.dumps(sim_state.movements, indent=2))\n",
    "    return {\"sim_state\": sim_state}\n",
    "\n",
    "def should_continue_simulation(state: State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    \n",
    "    # If there is no function call, then we finish\n",
    "    if sim_state.int_counter == 0:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def should_move(state: State):\n",
    "    int_counter = state[\"int_counter\"]\n",
    "    \n",
    "    # If there is no function call, then we finish\n",
    "    if int_counter == 0:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "def save(state:State):\n",
    "    sim_state = state[\"sim_state\"]\n",
    "    \n",
    "\n",
    "    # <sim_folder> points to the current simulation folder.\n",
    "    sim_folder = f\"{fs_storage}/{sim_state.sim_code}\"\n",
    "\n",
    "    # Save Reverie meta information.\n",
    "    reverie_meta = dict()\n",
    "    reverie_meta[\"fork_sim_code\"] = sim_state.fork_sim_code\n",
    "    reverie_meta[\"start_date\"] = sim_state.start_time.strftime(\"%B %d, %Y\")\n",
    "    reverie_meta[\"curr_time\"] = sim_state.curr_time.strftime(\"%B %d, %Y, %H:%M:%S\")\n",
    "    reverie_meta[\"sec_per_step\"] = sim_state.sec_per_step\n",
    "    reverie_meta[\"maze_name\"] = sim_state.maze.maze_name\n",
    "    reverie_meta[\"persona_names\"] = list(sim_state.personas.keys())\n",
    "    reverie_meta[\"step\"] = sim_state.step\n",
    "    reverie_meta_f = f\"{sim_folder}/reverie/meta.json\"\n",
    "    with open(reverie_meta_f, \"w\") as outfile:\n",
    "        outfile.write(json.dumps(reverie_meta, indent=2))\n",
    "\n",
    "    # Save the personas.\n",
    "    for persona_name, persona in sim_state.personas.items():\n",
    "        save_folder = f\"{sim_folder}/personas/{persona_name}/bootstrap_memory\"\n",
    "        persona.save(save_folder)\n",
    "\n",
    "    metrics.save()\n",
    "\n",
    "    return {\"sim_state\": sim_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ae66c-3b58-4283-a02a-36da72a2ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "workflow.add_node(\"input\", input)\n",
    "workflow.add_node(\"simulate\", simulate)\n",
    "workflow.add_node(\"output\", output)\n",
    "workflow.add_node(\"move\", move)\n",
    "\n",
    "workflow.set_entry_point(\"input\")\n",
    "workflow.add_edge(\"input\", \"simulate\")\n",
    "workflow.add_edge(\"simulate\", \"move\")\n",
    "workflow.add_edge(\"move\", \"output\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"output\",\n",
    "    should_continue_simulation,\n",
    "    {\n",
    "        \"continue\": \"input\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mind = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b369a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(mind.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb04b9-40b6-46f1-a7a8-4b2d8aba7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "maze = Maze(\"the_ville\")\n",
    "print(maze)\n",
    "config = {\"recursion_limit\": 10000000}\n",
    "sim_state = SimulationStates()\n",
    "\n",
    "sim_state.maze = maze\n",
    "sim_state.personas = {}\n",
    "sim_state.personas_tile = {}\n",
    "sim_state.fork_sim_code = 'notebook'\n",
    "sim_state.sim_code = \"notebook\"\n",
    "sim_state.sec_per_step = 5\n",
    "sim_state.step = 0\n",
    "sim_state.server_sleep = 0.1\n",
    "sim_state.int_counter = 1\n",
    "sim_state.frontend_data = {}\n",
    "sim_state.backend_data = {}\n",
    "\n",
    "state = await init({\"sim_state\": sim_state})\n",
    "\n",
    "sim_state.perceptions[\"Isabella Rodriguez\"] = [(\"the Ville:Isabella Rodriguez's apartment:main room:bed\", 'is', 'fire', \"bed is on fire\")]\n",
    "sim_state.perceptions[\"Klaus Mueller\"] = [(\"the Ville:Dorm for Oak Hill College:kitchen:refrigerator\", 'is', 'fire', \"refrigerator is on fire\")]\n",
    "\n",
    "sim_state = state[\"sim_state\"]\n",
    "\n",
    "inputs = {\"sim_state\": sim_state }\n",
    "state = await mind.ainvoke(inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception = await perceive(state[\"sim_state\"].personas[\"Isabella Rodriguez\"], state[\"sim_state\"].maze, [(\"the Ville:Isabella Rodriguez's apartment:main room:refrigerator\", 'is', 'broken', \"the Ville:Isabella Rodriguez's apartment:main room:refrigerator is broken\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception[0].spo_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception[0].spo_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"sim_state\"].maze.address_tiles[\"the Ville:Dorm for Oak Hill College:kitchen:refrigerator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    state[\"sim_state\"].int_counter = 1\n",
    "    inputs = {\"sim_state\": state[\"sim_state\"] }\n",
    "    state = await mind.ainvoke(inputs, config)\n",
    "    movements = state[\"sim_state\"].movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"sim_state\"].personas[\"Isabella Rodriguez\"].scratch.get_curr_event_and_desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d175612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import socketio\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import datetime\n",
    "import queue\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "sio = socketio.AsyncServer(async_mode='asgi', cors_allowed_origins=\"*\")\n",
    "app = socketio.ASGIApp(sio)\n",
    "\n",
    "# Queue to store the perceptions from the socket\n",
    "perception = dict()\n",
    "\n",
    "sleep_time = 5\n",
    "\n",
    "\n",
    "@sio.event\n",
    "async def connect(sid, data):\n",
    "    print('connect', sid)\n",
    "    # on connect send back a list of all the personas\n",
    "    # list_of_personas = list(state[\"sim_state\"].personas.keys())\n",
    "    # print(list_of_personas)\n",
    "    # await sio.emit('some', {\"personas\": list_of_personas})\n",
    "    print('connect 11', sid)\n",
    "\n",
    "@sio.event\n",
    "async def init(sid, data):\n",
    "    # on connect send back a list of all the personas\n",
    "    list_of_personas = list(state[\"sim_state\"].personas.keys())\n",
    "    print(list_of_personas)\n",
    "    await sio.emit('some', {\"personas\": list_of_personas})\n",
    "\n",
    "@sio.event\n",
    "async def percive(sid, data):\n",
    "    print('message', data)\n",
    "    if sid not in perception:\n",
    "        perception[sid] = queue.Queue()\n",
    "    perception[sid].put(data)\n",
    "\n",
    "@sio.event\n",
    "async def disconnect(sid):\n",
    "    print('disconnect', sid)\n",
    "\n",
    "async def generate_thoughts():\n",
    "    global state\n",
    "    while True:\n",
    "        await asyncio.sleep(sleep_time)  # 6 thoughts per minute -> one thought every 10 seconds\n",
    "        thought = f\"Generated thought at {datetime.datetime.now()}\"\n",
    "        print(thought)\n",
    "        \n",
    "        # convert perception dic of queues to dic of lists and keep the keys\n",
    "        \n",
    "        perception_list = {k: list(v.queue) for k, v in perception.items()}\n",
    "        print(perception_list)\n",
    "        \n",
    "        state[\"sim_state\"].int_counter = 1\n",
    "        inputs = {\"sim_state\": state[\"sim_state\"] }\n",
    "\n",
    "        # Measure the time taken for the mind.ainvoke call\n",
    "        start_time = time.time()\n",
    "        state = await mind.ainvoke(inputs, config)\n",
    "        end_time = time.time()\n",
    "\n",
    "        time_taken_ms = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        print(f\"mind.ainvoke call took {time_taken_ms:.2f} ms\")\n",
    "\n",
    "        # print(state)\n",
    "        print(state[\"sim_state\"].movements)\n",
    "        await sio.emit('act', state[\"sim_state\"].movements)\n",
    "        # await sio.emit('act', movements)\n",
    "        \n",
    "        # uncomment to save the state\n",
    "        # await save(state)\n",
    "        \n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(generate_thoughts())\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=3000)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await save(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb825f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cad76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749f130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622c2c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243b1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c05f8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df518a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0138fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f1f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ff594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceaabc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from contextlib import AbstractAsyncContextManager\n",
    "from types import TracebackType\n",
    "from typing import AsyncIterator, Optional\n",
    "\n",
    "import aiosqlite\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from typing_extensions import Self\n",
    "\n",
    "from langgraph.checkpoint.base import (\n",
    "    BaseCheckpointSaver,\n",
    "    Checkpoint,\n",
    "    CheckpointMetadata,\n",
    "    CheckpointTuple,\n",
    "    SerializerProtocol,\n",
    ")\n",
    "from langgraph.checkpoint.sqlite import JsonPlusSerializerCompat\n",
    "import jsonpickle\n",
    "\n",
    "class AsyncSqliteSaver(BaseCheckpointSaver, AbstractAsyncContextManager):\n",
    "    \"\"\"An asynchronous checkpoint saver that stores checkpoints in a SQLite database.\n",
    "\n",
    "    Tip:\n",
    "        Requires the [aiosqlite](https://pypi.org/project/aiosqlite/) package.\n",
    "        Install it with `pip install aiosqlite`.\n",
    "\n",
    "    Note:\n",
    "        While this class does support asynchronous checkpointing, it is not recommended\n",
    "        for production workloads, due to limitations in SQLite's write performance. For\n",
    "        production workloads, consider using a more robust database like PostgreSQL.\n",
    "\n",
    "    Args:\n",
    "        conn (aiosqlite.Connection): The asynchronous SQLite database connection.\n",
    "        serde (Optional[SerializerProtocol]): The serializer to use for serializing and deserializing checkpoints. Defaults to JsonPlusSerializerCompat.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        Usage within a StateGraph:\n",
    "\n",
    "            import asyncio\n",
    "            import aiosqlite\n",
    "\n",
    "            from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "            from langgraph.graph import StateGraph\n",
    "\n",
    "            builder = StateGraph(int)\n",
    "            builder.add_node(\"add_one\", lambda x: x + 1)\n",
    "            builder.set_entry_point(\"add_one\")\n",
    "            builder.set_finish_point(\"add_one\")\n",
    "\n",
    "            memory = AsyncSqliteSaver.from_conn_string(\"checkpoints.sqlite\")\n",
    "            graph = builder.compile(checkpointer=memory)\n",
    "            coro = graph.ainvoke(1, {\"configurable\": {\"thread_id\": \"thread-1\"}})\n",
    "            asyncio.run(coro)  # Output: 2\n",
    "\n",
    "\n",
    "        Raw usage:\n",
    "\n",
    "            import asyncio\n",
    "            import aiosqlite\n",
    "            from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "\n",
    "            async def main():\n",
    "                async with aiosqlite.connect(\"checkpoints.db\") as conn:\n",
    "                    saver = AsyncSqliteSaver(conn)\n",
    "                    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "                    checkpoint = {\"ts\": \"2023-05-03T10:00:00Z\", \"data\": {\"key\": \"value\"}}\n",
    "                    saved_config = await saver.aput(config, checkpoint)\n",
    "                    print(\n",
    "                        saved_config\n",
    "                    )  # Output: {\"configurable\": {\"thread_id\": \"1\", \"thread_ts\": \"2023-05-03T10:00:00Z\"}}\n",
    "\n",
    "\n",
    "            asyncio.run(main())\n",
    "    \"\"\"\n",
    "\n",
    "    serde = jsonpickle\n",
    "\n",
    "    conn: aiosqlite.Connection\n",
    "    lock: asyncio.Lock\n",
    "    is_setup: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conn: aiosqlite.Connection,\n",
    "        *,\n",
    "        serde: Optional[SerializerProtocol] = None,\n",
    "    ):\n",
    "        super().__init__(serde=serde)\n",
    "        self.conn = conn\n",
    "        self.lock = asyncio.Lock()\n",
    "        self.is_setup = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_conn_string(cls, conn_string: str) -> \"AsyncSqliteSaver\":\n",
    "        \"\"\"Create a new AsyncSqliteSaver instance from a connection string.\n",
    "\n",
    "        Args:\n",
    "            conn_string (str): The SQLite connection string.\n",
    "\n",
    "        Returns:\n",
    "            AsyncSqliteSaver: A new AsyncSqliteSaver instance.\n",
    "        \"\"\"\n",
    "        return AsyncSqliteSaver(conn=aiosqlite.connect(conn_string))\n",
    "\n",
    "    async def __aenter__(self) -> Self:\n",
    "        return self\n",
    "\n",
    "    async def __aexit__(\n",
    "        self,\n",
    "        __exc_type: Optional[type[BaseException]],\n",
    "        __exc_value: Optional[BaseException],\n",
    "        __traceback: Optional[TracebackType],\n",
    "    ) -> Optional[bool]:\n",
    "        if self.is_setup:\n",
    "            return await self.conn.close()\n",
    "\n",
    "    async def setup(self) -> None:\n",
    "        \"\"\"Set up the checkpoint database asynchronously.\n",
    "\n",
    "        This method creates the necessary tables in the SQLite database if they don't\n",
    "        already exist. It is called automatically when needed and should not be called\n",
    "        directly by the user.\n",
    "        \"\"\"\n",
    "        async with self.lock:\n",
    "            if self.is_setup:\n",
    "                return\n",
    "            if not self.conn.is_alive():\n",
    "                await self.conn\n",
    "            async with self.conn.executescript(\n",
    "                \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS checkpoints (\n",
    "                    thread_id TEXT NOT NULL,\n",
    "                    thread_ts TEXT NOT NULL,\n",
    "                    parent_ts TEXT,\n",
    "                    checkpoint BLOB,\n",
    "                    metadata BLOB,\n",
    "                    PRIMARY KEY (thread_id, thread_ts)\n",
    "                );\n",
    "                \"\"\"\n",
    "            ):\n",
    "                await self.conn.commit()\n",
    "\n",
    "            self.is_setup = True\n",
    "\n",
    "    async def aget_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n",
    "        \"\"\"Get a checkpoint tuple from the database asynchronously.\n",
    "\n",
    "        This method retrieves a checkpoint tuple from the SQLite database based on the\n",
    "        provided config. If the config contains a \"thread_ts\" key, the checkpoint with\n",
    "        the matching thread ID and timestamp is retrieved. Otherwise, the latest checkpoint\n",
    "        for the given thread ID is retrieved.\n",
    "\n",
    "        Args:\n",
    "            config (RunnableConfig): The config to use for retrieving the checkpoint.\n",
    "\n",
    "        Returns:\n",
    "            Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\n",
    "        \"\"\"\n",
    "        await self.setup()\n",
    "        if config[\"configurable\"].get(\"thread_ts\"):\n",
    "            async with self.conn.execute(\n",
    "                \"SELECT checkpoint, parent_ts, metadata FROM checkpoints WHERE thread_id = ? AND thread_ts = ?\",\n",
    "                (\n",
    "                    str(config[\"configurable\"][\"thread_id\"]),\n",
    "                    str(config[\"configurable\"][\"thread_ts\"]),\n",
    "                ),\n",
    "            ) as cursor:\n",
    "                if value := await cursor.fetchone():\n",
    "                    return CheckpointTuple(\n",
    "                        config,\n",
    "                        self.serde.decode(value[0]),\n",
    "                        self.serde.decode(value[2]) if value[2] is not None else {},\n",
    "                        {\n",
    "                            \"configurable\": {\n",
    "                                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                                \"thread_ts\": value[1],\n",
    "                            }\n",
    "                        }\n",
    "                        if value[1]\n",
    "                        else None,\n",
    "                    )\n",
    "        else:\n",
    "            async with self.conn.execute(\n",
    "                \"SELECT thread_id, thread_ts, parent_ts, checkpoint, metadata FROM checkpoints WHERE thread_id = ? ORDER BY thread_ts DESC LIMIT 1\",\n",
    "                (str(config[\"configurable\"][\"thread_id\"]),),\n",
    "            ) as cursor:\n",
    "                if value := await cursor.fetchone():\n",
    "                    return CheckpointTuple(\n",
    "                        {\n",
    "                            \"configurable\": {\n",
    "                                \"thread_id\": value[0],\n",
    "                                \"thread_ts\": value[1],\n",
    "                            }\n",
    "                        },\n",
    "                        self.serde.decode(value[3]),\n",
    "                        self.serde.decode(value[4]) if value[4] is not None else {},\n",
    "                        {\n",
    "                            \"configurable\": {\n",
    "                                \"thread_id\": value[0],\n",
    "                                \"thread_ts\": value[2],\n",
    "                            }\n",
    "                        }\n",
    "                        if value[2]\n",
    "                        else None,\n",
    "                    )\n",
    "\n",
    "    async def alist(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        *,\n",
    "        before: Optional[RunnableConfig] = None,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> AsyncIterator[CheckpointTuple]:\n",
    "        \"\"\"List checkpoints from the database asynchronously.\n",
    "\n",
    "        This method retrieves a list of checkpoint tuples from the SQLite database based\n",
    "        on the provided config. The checkpoints are ordered by timestamp in descending order.\n",
    "\n",
    "        Args:\n",
    "            config (RunnableConfig): The config to use for listing the checkpoints.\n",
    "            before (Optional[RunnableConfig]): If provided, only checkpoints before the specified timestamp are returned. Defaults to None.\n",
    "            limit (Optional[int]): The maximum number of checkpoints to return. Defaults to None.\n",
    "\n",
    "        Yields:\n",
    "            AsyncIterator[CheckpointTuple]: An asynchronous iterator of checkpoint tuples.\n",
    "        \"\"\"\n",
    "        await self.setup()\n",
    "        query = (\n",
    "            \"SELECT thread_id, thread_ts, parent_ts, checkpoint, metadata FROM checkpoints WHERE thread_id = ? ORDER BY thread_ts DESC\"\n",
    "            if before is None\n",
    "            else \"SELECT thread_id, thread_ts, parent_ts, checkpoint, metadata FROM checkpoints WHERE thread_id = ? AND thread_ts < ? ORDER BY thread_ts DESC\"\n",
    "        )\n",
    "        if limit:\n",
    "            query += f\" LIMIT {limit}\"\n",
    "        async with self.conn.execute(\n",
    "            query,\n",
    "            (\n",
    "                (str(config[\"configurable\"][\"thread_id\"]),)\n",
    "                if before is None\n",
    "                else (\n",
    "                    str(config[\"configurable\"][\"thread_id\"]),\n",
    "                    str(before[\"configurable\"][\"thread_ts\"]),\n",
    "                )\n",
    "            ),\n",
    "        ) as cursor:\n",
    "            async for thread_id, thread_ts, parent_ts, value, metadata in cursor:\n",
    "                yield CheckpointTuple(\n",
    "                    {\"configurable\": {\"thread_id\": thread_id, \"thread_ts\": thread_ts}},\n",
    "                    self.serde.decode(value),\n",
    "                    self.serde.decode(metadata) if metadata is not None else {},\n",
    "                    {\"configurable\": {\"thread_id\": thread_id, \"thread_ts\": parent_ts}}\n",
    "                    if parent_ts\n",
    "                    else None,\n",
    "                )\n",
    "\n",
    "    async def aput(\n",
    "        self,\n",
    "        config: RunnableConfig,\n",
    "        checkpoint: Checkpoint,\n",
    "        metadata: CheckpointMetadata,\n",
    "    ) -> RunnableConfig:\n",
    "        \"\"\"Save a checkpoint to the database asynchronously.\n",
    "\n",
    "        This method saves a checkpoint to the SQLite database. The checkpoint is associated\n",
    "        with the provided config and its parent config (if any).\n",
    "\n",
    "        Args:\n",
    "            config (RunnableConfig): The config to associate with the checkpoint.\n",
    "            checkpoint (Checkpoint): The checkpoint to save.\n",
    "\n",
    "        Returns:\n",
    "            RunnableConfig: The updated config containing the saved checkpoint's timestamp.\n",
    "        \"\"\"\n",
    "        await self.setup()\n",
    "        async with self.conn.execute(\n",
    "            \"INSERT OR REPLACE INTO checkpoints (thread_id, thread_ts, parent_ts, checkpoint, metadata) VALUES (?, ?, ?, ?, ?)\",\n",
    "            (\n",
    "                str(config[\"configurable\"][\"thread_id\"]),\n",
    "                checkpoint[\"ts\"],\n",
    "                config[\"configurable\"].get(\"thread_ts\"),\n",
    "                self.serde.encode(checkpoint),\n",
    "                self.serde.encode(metadata),\n",
    "            ),\n",
    "        ):\n",
    "            await self.conn.commit()\n",
    "        return {\n",
    "            \"configurable\": {\n",
    "                \"thread_id\": config[\"configurable\"][\"thread_id\"],\n",
    "                \"thread_ts\": checkpoint[\"ts\"],\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77014c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[\"sim_state\"].personas[\"Isabella Rodriguez\"].scratch.get_curr_event_and_desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "app_w_interrupt = workflow.compile(checkpointer=memory, interrupt_before=[\"simulate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e8155-70c5-4973-912c-dc55104b2acf",
   "metadata": {},
   "source": [
    "This may take a little bit - it's making a few calls behind the scenes.\n",
    "In order to start seeing some intermediate results as they happen, we can use streaming - see below for more information on that.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "LangGraph has support for several different types of streaming.\n",
    "\n",
    "### Streaming Node Output\n",
    "\n",
    "One of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544977e-31f7-41f0-88c4-ec9c27b8cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "maze = Maze(\"the_ville\")\n",
    "print(maze)\n",
    "inputs = {\"messages\": [HumanMessage(content=\"where is the answer\")], \"maze\": maze, \"personas\": {}, \"personas_tile\": {},\n",
    "\"fork_sim_code\": 'base_the_ville_isabella_maria_klaus', \"sim_code\" : \"notebook\", \"int_counter\": 5 }\n",
    "\n",
    "async for output in app_w_interrupt.astream(inputs, config, stream_mode=\"values\"):\n",
    "    # stream_mode=\"updates\" yields dictionaries with output keyed by node name\n",
    "    # for key, value in output.items():\n",
    "    #     print(f\"Output from node '{key}':\")\n",
    "    #     print(\"---\")\n",
    "    #     print(value[\"messages\"][-1].pretty_print())\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69caeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values = await app_w_interrupt.aget_state(config)\n",
    "current_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values[\"messages\"][-1].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bcb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values[\"messages\"][-1].tool_calls[0][\"args\"][\n",
    "    \"query\"\n",
    "] = \"weather in San Francisco today\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await app_w_interrupt.aupdate_state(config, current_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82918191",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values= await app_w_interrupt.aget_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ba870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for event in app_w_interrupt.astream(None, config):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b763b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values = app_w_interrupt.get_state(config)\n",
    "current_values.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonpickle\n",
    "\n",
    "\n",
    "json_string = jsonpickle.encode(maze)\n",
    "\n",
    "recreated_obj = jsonpickle.decode(json_string)\n",
    "print(recreated_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11949936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd140f0-a5a6-4697-8115-322242f197b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n",
    "async for output in app.astream_log(inputs, include_types=[\"llm\"]):\n",
    "    # astream_log() yields the requested logs (here LLMs) in JSONPatch format\n",
    "    for op in output.ops:\n",
    "        if op[\"path\"] == \"/streamed_output/-\":\n",
    "            # this is the output from .stream()\n",
    "            ...\n",
    "        elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n",
    "            \"/streamed_output/-\"\n",
    "        ):\n",
    "            # because we chose to only include LLMs, these are LLM tokens\n",
    "            print(op[\"value\"].content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae8246-11d5-40e1-8567-361e5bef8917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155ab57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    # base_url=base_api_url,\n",
    "    api_key='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',\n",
    ")\n",
    "model_name = \"gpt-3.5-turbo-0125\"\n",
    "def generate_thoughts_and_actions(perception_statements):\n",
    "    # Combine the perception statements into a single input prompt for the LLM\n",
    "    perception_input = \"\\n\".join(f\"- {statement}\" for statement in perception_statements)\n",
    "    \n",
    "    # Sensory Perception Layer\n",
    "    sensory_prompt = f\"\"\"\n",
    "    You are an advanced AI mimicking human consciousness. Given the following perception statements, interpret them as sensory inputs.\n",
    "\n",
    "    Perception statements:\n",
    "    {perception_input}\n",
    "\n",
    "    Example Sensory Interpretations:\n",
    "    - \"The sun is shining brightly.\" -> \"It's a sunny day.\"\n",
    "    - \"I hear birds chirping.\" -> \"Birds are active.\"\n",
    "    - \"The air smells like flowers.\" -> \"There are flowers nearby.\"\n",
    "\n",
    "    Sensory Interpretations:\n",
    "    \"\"\"\n",
    "    sensory_response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": sensory_prompt}],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=[\"Thought Processing:\"]\n",
    "    )\n",
    "    sensory_interpretations = sensory_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Thought Processing Layer\n",
    "    thought_prompt = f\"\"\"\n",
    "    Sensory Interpretations:\n",
    "    {sensory_interpretations}\n",
    "\n",
    "    Thought Processing:\n",
    "    Generate thoughts based on the above sensory interpretations.\n",
    "\n",
    "    Example Thoughts:\n",
    "    - \"It's a sunny day.\" -> \"The weather is nice.\"\n",
    "    - \"Birds are active.\" -> \"It might be morning.\"\n",
    "    - \"There are flowers nearby.\" -> \"Spring is in the air.\"\n",
    "\n",
    "    Thoughts:\n",
    "    \"\"\"\n",
    "    thought_response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": thought_prompt}],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=[\"Emotion Processing:\"]\n",
    "    )\n",
    "    thoughts = thought_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Emotion Processing Layer\n",
    "    emotion_prompt = f\"\"\"\n",
    "    Thoughts:\n",
    "    {thoughts}\n",
    "\n",
    "    Emotion Processing:\n",
    "    Generate emotional responses based on the above thoughts.\n",
    "\n",
    "    Example Emotions:\n",
    "    - \"The weather is nice.\" -> \"Happiness\"\n",
    "    - \"It might be morning.\" -> \"Excitement\"\n",
    "    - \"Spring is in the air.\" -> \"Contentment\"\n",
    "\n",
    "    Emotions:\n",
    "    \"\"\"\n",
    "    emotion_response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": emotion_prompt}],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=[\"Subconscious Processing:\"]\n",
    "    )\n",
    "    emotions = emotion_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Subconscious Processing Layer\n",
    "    subconscious_prompt = f\"\"\"\n",
    "    Emotions:\n",
    "    {emotions}\n",
    "\n",
    "    Subconscious Processing:\n",
    "    Generate subconscious influences based on the above emotions and thoughts.\n",
    "\n",
    "    Example Subconscious Influences:\n",
    "    - \"Happiness\" -> \"Feel energized\"\n",
    "    - \"Excitement\" -> \"Anticipate a good day\"\n",
    "    - \"Contentment\" -> \"Desire to enjoy the moment\"\n",
    "\n",
    "    Subconscious Influences:\n",
    "    \"\"\"\n",
    "    subconscious_response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": subconscious_prompt}],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=[\"Action Selection:\"]\n",
    "    )\n",
    "    subconscious_influences = subconscious_response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Action Selection Layer\n",
    "    action_prompt = f\"\"\"\n",
    "    Subconscious Influences:\n",
    "    {subconscious_influences}\n",
    "\n",
    "    Action Selection:\n",
    "    Generate appropriate actions based on all the previous layers.\n",
    "\n",
    "    Example Actions:\n",
    "    - \"Feel energized\" -> \"Decide to go for a walk\"\n",
    "    - \"Anticipate a good day\" -> \"Plan enjoyable activities\"\n",
    "    - \"Desire to enjoy the moment\" -> \"Take deep breaths and relax\"\n",
    "\n",
    "    Actions:\n",
    "    \"\"\"\n",
    "    action_response = openai_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": action_prompt}],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    actions = action_response.choices[0].message.content.strip()\n",
    "    \n",
    "    return {\n",
    "        \"Sensory Interpretations\": sensory_interpretations,\n",
    "        \"Thoughts\": thoughts,\n",
    "        \"Emotions\": emotions,\n",
    "        \"Subconscious Influences\": subconscious_influences,\n",
    "        \"Actions\": actions\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "perception_statements = [\n",
    "    \"The sun is shining brightly.\",\n",
    "    \"I hear birds chirping.\",\n",
    "    \"tv is on fire\"\n",
    "]\n",
    "\n",
    "result = generate_thoughts_and_actions(perception_statements)\n",
    "\n",
    "print(\"Structured Output:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(value)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3289b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
